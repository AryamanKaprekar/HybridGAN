{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:38:33.901044Z",
     "iopub.status.busy": "2025-11-13T15:38:33.900454Z",
     "iopub.status.idle": "2025-11-13T15:38:44.325472Z",
     "shell.execute_reply": "2025-11-13T15:38:44.324905Z",
     "shell.execute_reply.started": "2025-11-13T15:38:33.901020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(\"Device:\", device)\n",
    "\n",
    "data_dir = os.path.join(\"/kaggle\", \"input\", \"cityscapes-image-pairs\", \"cityscapes_data\")\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "train_fns = sorted(os.listdir(train_dir))\n",
    "val_fns = sorted(os.listdir(val_dir))\n",
    "print(\"Train files:\", len(train_fns), \"Val files:\", len(val_fns))\n",
    "\n",
    "sample_image_fp = os.path.join(train_dir, train_fns[0])\n",
    "sample_image = Image.open(sample_image_fp).convert(\"RGB\")\n",
    "plt.imshow(sample_image); plt.axis('off')\n",
    "print(\"sample:\", sample_image_fp)\n",
    "\n",
    "def split_image(image):\n",
    "    \"\"\"\n",
    "    image: numpy array H x W x 3\n",
    "    splits into left (cityscape) and right (label) halves.\n",
    "    \"\"\"\n",
    "    image = np.array(image)\n",
    "    h,w,_ = image.shape\n",
    "    assert w % 2 == 0, \"expected paired image with even width\"\n",
    "    mid = w // 2\n",
    "    cityscape = image[:, :mid, :]\n",
    "    label = image[:, mid:, :]\n",
    "    return cityscape, label\n",
    "\n",
    "sample_np = np.array(sample_image)\n",
    "print(\"sample shape:\", sample_np.shape)\n",
    "cityscape, label = split_image(sample_np)\n",
    "print(\"cityscape / label shapes:\", cityscape.shape, label.shape)\n",
    "\n",
    "\n",
    "MAX_IMAGES_TO_SAMPLE = 200  \n",
    "MAX_PIXELS = 20000           \n",
    "num_classes = 10\n",
    "\n",
    "colors = []\n",
    "collected = 0\n",
    "image_files = train_fns[:MAX_IMAGES_TO_SAMPLE]\n",
    "\n",
    "for fn in tqdm(image_files, desc=\"collecting label colors\"):\n",
    "    fp = os.path.join(train_dir, fn)\n",
    "    try:\n",
    "        img = Image.open(fp).convert(\"RGB\")\n",
    "        _, lbl = split_image(np.array(img))\n",
    "        h, w, _ = lbl.shape\n",
    "\n",
    "        pix = lbl.reshape(-1, 3)\n",
    "\n",
    "        remaining = MAX_PIXELS - collected\n",
    "        if remaining <= 0:\n",
    "            break\n",
    "        take = min(len(pix), max(1, remaining // (len(image_files))))\n",
    "        if take < len(pix):\n",
    "\n",
    "            idx = np.random.choice(len(pix), size=take, replace=False)\n",
    "            pix = pix[idx]\n",
    "        colors.append(pix)\n",
    "        collected += pix.shape[0]\n",
    "    except (UnidentifiedImageError, OSError) as e:\n",
    "        print(f\"skipping corrupt file {fp}: {e}\")\n",
    "        continue\n",
    "\n",
    "if len(colors) == 0:\n",
    "    raise RuntimeError(\"No colors collected from any training file. Check file paths and images.\")\n",
    "\n",
    "color_array = np.vstack(colors).astype(np.float32)\n",
    "print(\"color_array shape (for KMeans):\", color_array.shape)\n",
    "\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "label_model = MiniBatchKMeans(\n",
    "    n_clusters=num_classes,\n",
    "    random_state=42,\n",
    "    batch_size=4096,\n",
    "    n_init=\"auto\"\n",
    ")\n",
    "label_model.fit(color_array.astype(np.float64))  \n",
    "print(\"KMeans centroids shape:\", label_model.cluster_centers_.shape)\n",
    "\n",
    "\n",
    "cityscape, label = split_image(sample_np)\n",
    "label_class = label_model.predict(label.reshape(-1, 3).astype(np.float64)).reshape(label.shape[0], label.shape[1])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(cityscape); axes[0].set_title(\"city\")\n",
    "axes[1].imshow(label); axes[1].set_title(\"raw label\")\n",
    "axes[2].imshow(label_class); axes[2].set_title(\"kmeans class\")\n",
    "for ax in axes: ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "class CityscapeDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_model, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_fns = sorted(os.listdir(image_dir))\n",
    "        self.label_model = label_model\n",
    "        self.transform = transform if transform is not None else transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                 std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_fns)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_fn = self.image_fns[index]\n",
    "        image_fp = os.path.join(self.image_dir, image_fn)\n",
    "        try:\n",
    "            image = Image.open(image_fp).convert('RGB')\n",
    "            image = np.array(image)\n",
    "        except (UnidentifiedImageError, OSError) as e:\n",
    "\n",
    "            H = 256; W = 256\n",
    "            cityscape = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "            label_class = np.zeros((H, W), dtype=np.int64)\n",
    "            return self.transform(cityscape), torch.from_numpy(label_class).long()\n",
    "\n",
    "        cityscape, label = split_image(image)\n",
    "\n",
    "        pix = label.reshape(-1, 3).astype(np.float32)\n",
    "        cls = self.label_model.predict(pix.astype(np.float64)).reshape(label.shape[0], label.shape[1])\n",
    "\n",
    "        city_t = self.transform(cityscape)  \n",
    "        label_t = torch.from_numpy(cls).long()  \n",
    "        return city_t, label_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:38:50.577222Z",
     "iopub.status.busy": "2025-11-13T15:38:50.576338Z",
     "iopub.status.idle": "2025-11-13T15:38:54.186515Z",
     "shell.execute_reply": "2025-11-13T15:38:54.185862Z",
     "shell.execute_reply.started": "2025-11-13T15:38:50.577196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Generator\n",
    "# -------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# -------------------------------\n",
    "# Feature Pyramid Network (FPN)\n",
    "# -------------------------------\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self, in_channels_list, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lateral_convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_ch, out_channels, 1) for in_ch in in_channels_list\n",
    "        ])\n",
    "\n",
    "        self.smooth_convs = nn.ModuleList([\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "            for _ in in_channels_list\n",
    "        ])\n",
    "\n",
    "    def forward(self, features):\n",
    "\n",
    "        last_inner = self.lateral_convs[-1](features[-1]) \n",
    "        results = [last_inner]\n",
    "\n",
    "        for i in range(len(features) - 2, -1, -1):\n",
    "            lateral = self.lateral_convs[i](features[i])\n",
    "            inner_up = F.interpolate(last_inner, size=lateral.shape[-2:], mode=\"nearest\")\n",
    "            last_inner = lateral + inner_up\n",
    "            results.insert(0, last_inner)\n",
    "\n",
    "        results = [smooth(x) for smooth, x in zip(self.smooth_convs, results)]\n",
    "        return results  \n",
    "\n",
    "# -------------------------------\n",
    "# ASPP \n",
    "# -------------------------------\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, rates=(6, 12, 18)):\n",
    "        super().__init__()\n",
    "        blocks = []\n",
    "        blocks.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ))\n",
    "        for r in rates:\n",
    "            blocks.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=r, dilation=r, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "        self.atrous_blocks = nn.ModuleList(blocks)\n",
    "        self.global_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * (len(rates) + 2), out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[2:]\n",
    "        res = [m(x) for m in self.atrous_blocks]\n",
    "        gp = self.global_pool(x)\n",
    "        gp = F.interpolate(gp, size=size, mode='bilinear', align_corners=False)\n",
    "        x = torch.cat(res + [gp], dim=1)\n",
    "        return self.project(x)\n",
    "\n",
    "# -------------------------------\n",
    "# Decoder \n",
    "# -------------------------------\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.classifier = nn.Conv2d(mid_channels, num_classes, 1)\n",
    "\n",
    "    def forward(self, x, target_size=None):\n",
    "        x = self.conv1(x)\n",
    "        x = self.classifier(x)\n",
    "        if target_size is not None:\n",
    "            x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "# -------------------------------\n",
    "# Generator: ResNet50 + FPN + ASPP + Decoder\n",
    "# -------------------------------\n",
    "class FPN_ASPP_Generator(nn.Module):\n",
    "    def __init__(self, num_classes=10, use_imagenet_weights=True, fpn_out=256, aspp_out=256, decoder_mid=128):\n",
    "        super().__init__()\n",
    "        self.backbone = create_model(\n",
    "            'efficientnet_b0',          \n",
    "            pretrained=use_imagenet_weights,\n",
    "            features_only=True,        \n",
    "            out_indices=(1, 2, 3, 4)    \n",
    "        )\n",
    "        in_channels_list = self.backbone.feature_info.channels()\n",
    "        self.fpn = FPN(in_channels_list, fpn_out)\n",
    "        self.aspp = ASPP(fpn_out, aspp_out)\n",
    "        self.decoder = Decoder(aspp_out, decoder_mid, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[2:]\n",
    "        features = self.backbone(x)\n",
    "        c2, c3, c4, c5 = features\n",
    "        fpn_feats = self.fpn([c2, c3, c4, c5])\n",
    "        x = fpn_feats[0]\n",
    "        x = self.aspp(x)\n",
    "        x = self.decoder(x, target_size=size)\n",
    "        return x\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# PatchGAN Discriminator \n",
    "# -------------------------------\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=4, ndf=64, n_layers=3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        nf = ndf\n",
    "        layers += [nn.Conv2d(in_channels, nf, 4, 2, 1), nn.LeakyReLU(0.2, True)]\n",
    "        for _ in range(1, n_layers):\n",
    "            prev = nf\n",
    "            nf = min(nf * 2, 512)\n",
    "            layers += [\n",
    "                nn.Conv2d(prev, nf, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(nf),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "        nf2 = min(nf * 2, 512)\n",
    "        layers += [\n",
    "            nn.Conv2d(nf, nf2, 4, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(nf2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "        layers += [nn.Conv2d(nf2, 1, 4, 1, 1)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# -------------------------------\n",
    "# Hinge GAN losses\n",
    "# -------------------------------\n",
    "def hinge_d_loss(real_logits, fake_logits):\n",
    "    loss_real = torch.mean(F.relu(1.0 - real_logits))\n",
    "    loss_fake = torch.mean(F.relu(1.0 + fake_logits))\n",
    "    return loss_real + loss_fake\n",
    "\n",
    "def hinge_g_loss(fake_logits):\n",
    "    return -torch.mean(fake_logits)\n",
    "\n",
    "# -------------------------------\n",
    "# SegmentationGAN wrapper\n",
    "# -------------------------------\n",
    "class SegmentationGAN(nn.Module):\n",
    "    def __init__(self, num_classes=10, use_imagenet_weights=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.generator = FPN_ASPP_Generator(num_classes=num_classes, use_imagenet_weights=use_imagenet_weights)\n",
    "        self.discriminator = PatchDiscriminator(in_channels=3 + 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_aux_from_prob(prob):\n",
    "\n",
    "        aux = prob.max(dim=1, keepdim=True).values\n",
    "        return aux\n",
    "\n",
    "    @staticmethod\n",
    "    def build_aux_from_mask(masks, num_classes):\n",
    "        \n",
    "        b, h, w = masks.shape[0], masks.shape[1], masks.shape[2]\n",
    "        return torch.ones((b, 1, h, w), device=masks.device, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        seg_logits = self.generator(imgs)\n",
    "        prob = F.softmax(seg_logits, dim=1)\n",
    "        aux_fake = self.build_aux_from_prob(prob)\n",
    "        disc_in = torch.cat([imgs, aux_fake], dim=1)\n",
    "        disc_logits = self.discriminator(disc_in)\n",
    "        return seg_logits, disc_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T16:19:25.437000Z",
     "iopub.status.busy": "2025-11-13T16:19:25.436695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 5-Fold Cross Validation \n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------- Hyperparameters ----------\n",
    "num_classes = 10              \n",
    "batch_size = 16\n",
    "num_epochs = 10               \n",
    "lr = 1e-3\n",
    "weight_decay = 0\n",
    "num_folds = 5\n",
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "def compute_confusion_matrix(preds, labels, num_classes):\n",
    "    mask = (labels >= 0) & (labels < num_classes)\n",
    "    hist = np.bincount(num_classes * labels[mask].astype(int) + preds[mask].astype(int),\n",
    "                       minlength=num_classes**2).reshape(num_classes, num_classes)\n",
    "    return hist\n",
    "\n",
    "def compute_metrics_from_confusion(conf):\n",
    "    epsilon = 1e-10\n",
    "    true_positive = np.diag(conf).astype(np.float64)\n",
    "    gt_total = conf.sum(axis=1).astype(np.float64)\n",
    "    pred_total = conf.sum(axis=0).astype(np.float64)\n",
    "    union = gt_total + pred_total - true_positive\n",
    "    iou = true_positive / (union + epsilon)\n",
    "    per_class_acc = true_positive / (gt_total + epsilon)\n",
    "    valid = gt_total > 0\n",
    "    mIoU = np.mean(iou[valid]) if valid.any() else 0.0\n",
    "    mPA = np.mean(per_class_acc[valid]) if valid.any() else 0.0\n",
    "    return {'iou_per_class': iou, 'acc_per_class': per_class_acc, 'mIoU': float(mIoU), 'mPA': float(mPA)}\n",
    "\n",
    "# ---------- Dataset ----------\n",
    "train_dataset_full = CityscapeDataset(train_dir, label_model)\n",
    "test_dataset = CityscapeDataset(val_dir, label_model)\n",
    "print(f\"Train dataset size: {len(train_dataset_full)}, Test (val_dir) size: {len(test_dataset)}\")\n",
    "# ---------- GAN Training Utils ----------\n",
    "def d_step(discriminator, real_imgs, fake_aux, real_aux, opt_d):\n",
    "    \"\"\"\n",
    "    Train discriminator with hinge loss on real (img + real_aux) and fake (img + fake_aux).\n",
    "    \"\"\"\n",
    "    discriminator.train()\n",
    "    opt_d.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Real logits\n",
    "    real_in = torch.cat([real_imgs, real_aux], dim=1)  \n",
    "    real_logits = discriminator(real_in)\n",
    "\n",
    "    # Fake logits\n",
    "    fake_in = torch.cat([real_imgs, fake_aux], dim=1)\n",
    "    fake_logits = discriminator(fake_in.detach())\n",
    "\n",
    "    # Hinge D loss\n",
    "    d_loss = hinge_d_loss(real_logits, fake_logits)\n",
    "    d_loss.backward()\n",
    "    opt_d.step()\n",
    "    return d_loss.item()\n",
    "\n",
    "def g_step(generator, discriminator, imgs, masks, opt_g, ce_loss, num_classes):\n",
    "    \"\"\"\n",
    "    Train generator with CE segmentation loss + adversarial hinge loss.\n",
    "    \"\"\"\n",
    "    generator.train()\n",
    "    opt_g.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Forward generator\n",
    "    seg_logits = generator(imgs)                       \n",
    "    ce = ce_loss(seg_logits, masks)                   \n",
    "\n",
    "    # Build aux from probs for adversarial path\n",
    "    with torch.no_grad():\n",
    "        prob = F.softmax(seg_logits, dim=1)\n",
    "        aux_fake = SegmentationGAN.build_aux_from_prob(prob)  \n",
    "\n",
    "    disc_in = torch.cat([imgs, aux_fake], dim=1)\n",
    "    fake_logits = discriminator(disc_in)\n",
    "\n",
    "    adv_g = hinge_g_loss(fake_logits)\n",
    "\n",
    "    # Total G loss: CE + lambda_adv * adversarial\n",
    "    lambda_adv = 0.2\n",
    "    g_loss = ce + lambda_adv * adv_g\n",
    "    g_loss.backward()\n",
    "    opt_g.step()\n",
    "\n",
    "    return g_loss.item(), ce.item(), adv_g.item()\n",
    "\n",
    "# ---------- One epoch over a loader ----------\n",
    "def train_one_epoch(model_gan, loader, opt_g, opt_d, ce_loss, device, num_classes, fold, epoch, total_epochs):\n",
    "    model_gan.train()\n",
    "    gen = model_gan.generator\n",
    "    disc = model_gan.discriminator\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Fold {fold} | Epoch {epoch}/{total_epochs} [GAN Train]\", leave=False)\n",
    "    running = {\"D\": 0.0, \"G\": 0.0, \"CE\": 0.0, \"G_adv\": 0.0}\n",
    "    n = 0\n",
    "\n",
    "    for imgs, masks in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "        seg_logits = gen(imgs)\n",
    "        with torch.no_grad():\n",
    "            prob = F.softmax(seg_logits, dim=1)\n",
    "            aux_fake = SegmentationGAN.build_aux_from_prob(prob)           \n",
    "            aux_real = SegmentationGAN.build_aux_from_mask(masks, num_classes)  \n",
    "\n",
    "        # 1) D step (hinge)\n",
    "        d_loss = d_step(disc, imgs, aux_fake, aux_real, opt_d)\n",
    "\n",
    "        # 2) G step (CE + adversarial hinge)\n",
    "        g_loss, ce, g_adv = g_step(gen, disc, imgs, masks, opt_g, ce_loss, num_classes)\n",
    "        bsz = imgs.size(0)\n",
    "        running[\"D\"] += d_loss * bsz\n",
    "        running[\"G\"] += g_loss * bsz\n",
    "        running[\"CE\"] += ce * bsz\n",
    "        running[\"G_adv\"] += g_adv * bsz\n",
    "        n += bsz\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"D\": f\"{running['D']/n:.3f}\",\n",
    "            \"G\": f\"{running['G']/n:.3f}\",\n",
    "            \"CE\": f\"{running['CE']/n:.3f}\",\n",
    "            \"G_adv\": f\"{running['G_adv']/n:.3f}\",\n",
    "        })\n",
    "    for k in running:\n",
    "        running[k] = running[k] / max(1, n)\n",
    "    return running\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model_on_loader(model_gan, loader, device, num_classes):\n",
    "    model_gan.eval()\n",
    "    def forward_fn(x):\n",
    "        return model_gan.generator(x)\n",
    "    conf = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "    for imgs, masks in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device)\n",
    "        logits = forward_fn(imgs)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        gts = masks.cpu().numpy()\n",
    "        for p, g in zip(preds, gts):\n",
    "            conf += compute_confusion_matrix(p.flatten(), g.flatten(), num_classes=num_classes)\n",
    "    metrics = compute_metrics_from_confusion(conf)\n",
    "    return metrics, conf\n",
    "\n",
    "\n",
    "# ---------- K-Fold Cross Validation ----------\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "indices = np.arange(len(train_dataset_full))\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(indices), 1):\n",
    "    print(f\"\\n========== Fold {fold}/{num_folds} ==========\")\n",
    "\n",
    "    train_subset = Subset(train_dataset_full, train_idx)\n",
    "    val_subset = Subset(train_dataset_full, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # Build GAN (generator + patch discriminator)\n",
    "    gan = SegmentationGAN(num_classes=num_classes, use_imagenet_weights=True).to(device)\n",
    "\n",
    "    # Two optimizers: one for G, one for D\n",
    "    opt_g = optim.Adam(gan.generator.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    opt_d = optim.Adam(gan.discriminator.parameters(), lr=1e-5, weight_decay=weight_decay)\n",
    "\n",
    "    # CE loss for segmentation \n",
    "    ce_loss = nn.CrossEntropyLoss()  \n",
    "\n",
    "    best_val_mIoU = -1.0\n",
    "    best_state = None\n",
    "\n",
    "    epoch_iter = tqdm(range(1, num_epochs + 1), desc=f\"Fold {fold} GAN Training\", leave=False)\n",
    "    for epoch in epoch_iter:\n",
    "        train_stats = train_one_epoch(\n",
    "            gan, train_loader, opt_g, opt_d, ce_loss, device, num_classes, fold, epoch, num_epochs\n",
    "        )\n",
    "\n",
    "        # Evaluate generator segmentation performance on fold-val\n",
    "        val_metrics, _ =evaluate_model_on_loader(gan, val_loader, device, num_classes)\n",
    "        val_mIoU = val_metrics['mIoU']\n",
    "        val_mPA = val_metrics['mPA']\n",
    "\n",
    "        epoch_iter.set_postfix({\n",
    "            \"D\": f\"{train_stats['D']:.3f}\",\n",
    "            \"G\": f\"{train_stats['G']:.3f}\",\n",
    "            \"CE\": f\"{train_stats['CE']:.3f}\",\n",
    "            \"G_adv\": f\"{train_stats['G_adv']:.3f}\",\n",
    "            \"Val_mIoU\": f\"{val_mIoU:.4f}\",\n",
    "            \"Val_mPA\": f\"{val_mPA:.4f}\"\n",
    "        })\n",
    "\n",
    "        print(f\"Fold {fold} | Epoch {epoch:02d}/{num_epochs} -> \"\n",
    "              f\"D: {train_stats['D']:.3f} | G: {train_stats['G']:.3f} | \"\n",
    "              f\"CE: {train_stats['CE']:.3f} | G_adv: {train_stats['G_adv']:.3f} | \"\n",
    "              f\"Val mIoU: {val_mIoU:.4f} | Val mPA: {val_mPA:.4f}\")\n",
    "\n",
    "        if val_mIoU > best_val_mIoU:\n",
    "            best_val_mIoU = val_mIoU\n",
    "            best_state = {\n",
    "                \"gen\": copy.deepcopy(gan.generator.state_dict()),\n",
    "                \"disc\": copy.deepcopy(gan.discriminator.state_dict())\n",
    "            }\n",
    "\n",
    "    if best_state is not None:\n",
    "        gan.generator.load_state_dict(best_state[\"gen\"])\n",
    "        gan.discriminator.load_state_dict(best_state[\"disc\"])\n",
    "\n",
    "    test_metrics, _ = evaluate_model_on_loader(gan, test_loader, device, num_classes)\n",
    "    print(f\"✅ Fold {fold} TEST results -> mIoU: {test_metrics['mIoU']:.4f}, mPA: {test_metrics['mPA']:.4f}\")\n",
    "    fold_metrics.append(test_metrics)\n",
    "\n",
    "\n",
    "# ---------- Results Summary ----------\n",
    "num_params_gen = sum(p.numel() for p in gan.generator.parameters() if p.requires_grad)\n",
    "num_params_disc = sum(p.numel() for p in gan.discriminator.parameters() if p.requires_grad)\n",
    "total_params = num_params_gen + num_params_disc\n",
    "\n",
    "print(f\"Generator parameters: {num_params_gen:,}\")\n",
    "print(f\"Discriminator parameters: {num_params_disc:,}\")\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "all_mIoU = [m['mIoU'] for m in fold_metrics]\n",
    "all_mPA = [m['mPA'] for m in fold_metrics]\n",
    "print(\"\\n===== Cross-validation summary =====\")\n",
    "for i, (miou, mpa) in enumerate(zip(all_mIoU, all_mPA), 1):\n",
    "    print(f\"Fold {i} -> mIoU: {miou:.4f}  mPA: {mpa:.4f}\")\n",
    "print(f\"\\nAverage mIoU over {num_folds} folds: {np.mean(all_mIoU):.4f} ± {np.std(all_mIoU):.4f}\")\n",
    "print(f\"Average mPA  over {num_folds} folds: {np.mean(all_mPA):.4f} ± {np.std(all_mPA):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 22655,
     "sourceId": 29047,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
